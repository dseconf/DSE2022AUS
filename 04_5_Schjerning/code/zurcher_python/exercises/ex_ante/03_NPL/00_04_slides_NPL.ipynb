{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "round-collar",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plan for today:\n",
    "\n",
    "Solve exercise 5\n",
    "\n",
    "* We will look at the paper \"Swapping the Nested Fixed Point Algorithm: A Class of Estimators for Discrete Markov Decision Models\", where Aguirregabiria and Mira derive the Nested Pseudo Likelihood algorithm\n",
    "* Use the Nested Pseudo Likelihood to solve the engine replacement model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-burns",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 5: The Engine Replacement Model using Nested Pseudo Likelihood\n",
    "\n",
    "Consider the engine replacement model given by:\n",
    "\n",
    "$$\n",
    "V(x,\\varepsilon) = \\max_{d\\in \\{0,1\\}} \\big\\{ u(x,d) + \\varepsilon_d + \\beta\n",
    "\\underbrace{\\int_{X} \\int_{\\Omega} V(x',\\varepsilon') \\pi(x'|x,d) q(\\varepsilon'|x') dx' d\\varepsilon' }_{EV(x,d)} \\big\\}\n",
    "$$\n",
    "\n",
    "Where $ \\varepsilon $ is extreme value Type I distribued and utility is given by:\n",
    "\n",
    "$$\n",
    "u(x,d)=\\left \\{\n",
    "\\begin{array}{ll}\n",
    "    -RC-c(0,\\theta_1) & \\text{if }d=\\text{replace}=1 \\\\\n",
    "    -c(x,\\theta_1) & \\text{if }d=\\text{keep}=0\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Here\n",
    "\n",
    "- $ RC $ = replacement cost  \n",
    "- $ c(x,\\theta_1)  = \\theta_1 x$, cost of maintenance with preference parameters $ \\theta_1 $  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-journalism",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall the conditional choice probability (CCP) is given by:\n",
    "\n",
    "$$\n",
    "P(0|x) = \\frac{\\exp[ u(x,0) + \\beta EV(x,0)  ]}{\\sum_{d\\in \\{0,1\\}} \\exp[u(x,d) + \\beta EV(x,d)]}=\\frac{\\exp[ u(x,0) + \\beta \\int_{X} \\int_{\\Omega} V(x',\\varepsilon') \\pi(x'|x,d) q(\\varepsilon'|x') dx' d\\varepsilon'   ]}{\\sum_{d\\in \\{0,1\\}} \\exp[u(x,d) + \\beta \\int_{X} \\int_{\\Omega} V(x',\\varepsilon') \\pi(x'|x,d) q(\\varepsilon'|x') dx' d\\varepsilon' ]}\n",
    "$$\n",
    "\n",
    "In each iteration of the likelihood function, we had to solve the integral:\n",
    "\n",
    "$$EV(x,0)=\\int_{X} \\int_{\\Omega} V(x',\\varepsilon') \\pi(x'|x,d) q(\\varepsilon'|x') dx' d\\varepsilon'$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-spirit",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What do they do in the Nested Pseudo Likelihood (NPL) algorithm?\n",
    "\n",
    "Notice, normally we go from $\\tilde{v}(x,a) \\rightarrow P(a|x)$:\n",
    "\n",
    "$$P(a|x) = \\frac{\\exp[ \\tilde{v}(x,a) ]}{\\sum_{d\\in \\{0,1\\}} \\exp[\\tilde{v}(x,a)]} $$\n",
    "\n",
    "This forces us to solve the integral to get the value for $\\tilde{v}(x,a)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-princess",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hotz and Miller, 1993, showed that we can go the other way around (the inversion theorem):\n",
    "\n",
    "$$P(a|x) = \\frac{\\exp[ \\tilde{v}(x,a) ]}{\\sum_{d\\in \\{0,1\\}} \\exp[\\tilde{v}(x,a)]} $$\n",
    "\n",
    "Meaning if you give me the $P(a | x)$ I can give you the $\\tilde{v}(x,a)$:\n",
    "\n",
    "$$ P(a|x) \\rightarrow \\tilde{v}(x,a)$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-railway",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why is the Hotz and Miller inversion useful?\n",
    "\n",
    "We can use the inversion teorem to go from $P \\rightarrow P$:\n",
    "\n",
    "$$P=\\Lambda (\\varphi(P))=\\Psi (P)$$\n",
    "\n",
    "In this way, we avoid solving the inner fixed point, i.e. the integral. Instead we can just solve this new Bellman equation for $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-special",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spelling out the Nested Pseudo Likelihood (NPL) algorithm:\n",
    "\n",
    "Initialize algorithm:\n",
    "\n",
    "* Start with initial values for the transition probabilities\n",
    "\n",
    "* Start with an initial guess on the choice probabilities, $P^0$ and structural parameters $\\alpha_0=(RC, \\theta_1)$:\n",
    "\n",
    "The NPL algorithm goes as follows:\n",
    "\n",
    "* Step 1: Maximize the log likelihood function. $$\\alpha^K = \\arg \\max_{\\alpha} \\sum_{i=1}^n \\ln \\Psi_{\\alpha} (P^{K-1})$$\n",
    "\n",
    "* Step 2: Update CCP with new $\\alpha^K$ estimate:\n",
    "\n",
    "$$P^K=\\Psi_{\\alpha_K} (P^{K-1})$$\n",
    "\n",
    "Repeat step 1 and 2 until $P$ and $\\alpha$ do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-growth",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Now it is your turn to code.\n",
    "\n",
    "* You will find all the formulas for the model on Bertel's slides. Getting the formulas right is the hard part of the exercise."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
